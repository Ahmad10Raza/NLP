{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Processing: NLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLP Text Processing\n",
    "\n",
    "Text processing in the context of NLP (Natural Language Processing) refers to a set of techniques and operations applied to textual data to make it more accessible and useful for various NLP tasks. It involves the manipulation, analysis, and transformation of text to extract valuable information or gain insights. Text processing is a crucial preliminary step in many NLP applications. Here are some common text processing tasks:\n",
    "\n",
    "1. **Tokenization:** Tokenization involves splitting a text into individual units, such as words or sentences. These units are known as tokens. Tokenization is a fundamental step in NLP, as it breaks down text into manageable components.\n",
    "\n",
    "2. **Stemming and Lemmatization:** Stemming and lemmatization are techniques to reduce words to their root or base forms. Stemming removes prefixes and suffixes, while lemmatization considers the word's context and grammar to find its base form.\n",
    "\n",
    "3. **Stop Word Removal:** Stop words are common words like \"the,\" \"and,\" \"in,\" which may not provide valuable information in certain NLP tasks. Removing stop words can reduce noise in the text.\n",
    "\n",
    "4. **Normalization:** Text normalization involves standardizing text, making it consistent by converting all text to lowercase, removing special characters, and handling abbreviations or acronyms.\n",
    "\n",
    "5. **Text Cleaning:** Text cleaning aims to remove noise or irrelevant information from text. This may include removing HTML tags, punctuation, or unwanted characters.\n",
    "\n",
    "6. **Sentence Segmentation:** Sentence segmentation involves identifying sentence boundaries in a paragraph of text. This is important for tasks like machine translation or summarization.\n",
    "\n",
    "7. **Text Encoding:** Text encoding converts text into a numerical format that machine learning models can work with. This is typically done using techniques like one-hot encoding or word embeddings (e.g., Word2Vec or GloVe).\n",
    "\n",
    "8. **Part-of-Speech Tagging:** This task involves labeling words in a sentence with their part of speech (e.g., noun, verb, adjective). It's useful for understanding the grammatical structure of text.\n",
    "\n",
    "9. **Named Entity Recognition (NER):** NER identifies and classifies entities in text, such as names of people, organizations, locations, dates, and more.\n",
    "\n",
    "10. **Sentiment Analysis:** Sentiment analysis determines the sentiment or emotion expressed in a piece of text, typically classifying it as positive, negative, or neutral.\n",
    "\n",
    "11. **Text Summarization:** Text summarization techniques aim to create a concise summary of a longer text while retaining its essential information.\n",
    "\n",
    "12. **Topic Modeling:** Topic modeling algorithms can identify topics or themes within a collection of documents.\n",
    "\n",
    "13. **Dependency Parsing:** Dependency parsing analyzes the grammatical structure of a sentence to identify the relationships between words.\n",
    "\n",
    "14. **Text Translation:** Machine translation systems use NLP techniques to translate text from one language to another.\n",
    "\n",
    "Text processing plays a crucial role in NLP because it prepares raw textual data for subsequent analysis, modeling, and understanding. The specific techniques used depend on the NLP task at hand and the nature of the text data being processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String `Tokenization`\n",
    "\n",
    "String tokenization is a fundamental process in Natural Language Processing (NLP) that involves breaking down a text or a sequence of characters (often a sentence or a document) into smaller, meaningful units called tokens. Tokens are usually words, subwords, or even individual characters, depending on the level of granularity desired for text analysis. Tokenization is the first step in preparing textual data for further analysis and is crucial for many NLP tasks. Here's an overview of string tokenization in NLP:\n",
    "\n",
    "1. **Token Types:**\n",
    "   - **Word Tokenization:** In this most common form of tokenization, text is divided into individual words or terms. For example, the sentence \"The quick brown fox\" would be tokenized into the tokens: [\"The\", \"quick\", \"brown\", \"fox\"].\n",
    "   - **Subword Tokenization:** Some NLP tasks, like language modeling or handling languages with complex morphology, require breaking words into subword units. This is often done using techniques like Byte-Pair Encoding (BPE), SentencePiece, or WordPiece, which create subword tokens like \"subword\", \"##word\" for \"subword\" in English.\n",
    "   - **Character Tokenization:** In some cases, especially in languages without clear word boundaries or for character-level tasks, tokenization may occur at the character level. For example, \"apple\" could be tokenized as [\"a\", \"p\", \"p\", \"l\", \"e\"].\n",
    "\n",
    "2. **Token Boundaries:**\n",
    "   - **Whitespace Tokenization:** The most basic form of tokenization splits text on spaces, tabs, or newlines. However, this can lead to issues in languages where compound words are common or in languages without spaces between words.\n",
    "   - **Punctuation Tokenization:** Text can also be tokenized based on punctuation marks, which works well for many languages. For example, \"I am happy!\" would be tokenized into [\"I\", \"am\", \"happy\", \"!\"].\n",
    "   - **Specialized Tokenization:** Depending on the task and language, tokenization can be tailored to the specific needs. For instance, for some languages, you might tokenize based on specific character sequences or linguistic rules.\n",
    "\n",
    "3. **Tokenization Libraries:** Various NLP libraries and tools offer tokenization functions, such as spaCy, NLTK, the Natural Language Toolkit, or the tokenization tools provided by machine learning frameworks like TensorFlow or PyTorch.\n",
    "\n",
    "4. **Preprocessing:** Tokenization is often a crucial part of text preprocessing in NLP, preparing the text for downstream tasks like sentiment analysis, machine translation, text classification, and named entity recognition.\n",
    "\n",
    "5. **Challenges:** Tokenization can be challenging in languages with complex word structures, morphological variations, and ambiguous word boundaries. For these cases, advanced techniques and language-specific tokenizers are employed.\n",
    "\n",
    "6. **Post-processing:** After tokenization, you may need to further process the tokens, such as removing stopwords (common words like \"the,\" \"a\") or stemming (reducing words to their root form).\n",
    "\n",
    "Tokenization is a critical step in NLP because it structures the text data into manageable units, making it suitable for various language analysis and machine learning tasks. The choice of tokenization method and granularity depends on the specific NLP task and the characteristics of the language being processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'an', 'important', 'step', 'in', 'NLP', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/blackheart/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download the necessary data for tokenization\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenization is an important step in NLP.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Hugging Face's `tokenizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tokenizers in /home/blackheart/.local/lib/python3.10/site-packages (0.13.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Sentences Processing in NLP?\n",
    "\n",
    "Sentence processing in Natural Language Processing (NLP) refers to the analysis and understanding of individual sentences or text fragments within a larger body of text. It involves various linguistic and computational techniques to extract meaning, syntactic structure, and context from sentences. Sentence processing is a fundamental step in many NLP applications, including text classification, sentiment analysis, machine translation, and question-answering systems. Here are key aspects of sentence processing:\n",
    "\n",
    "1. **Tokenization:** The first step in sentence processing is often tokenization, where a sentence is divided into smaller units called tokens. Tokens can be words, subwords, or characters, depending on the specific requirements of the NLP task. Tokenization helps create a structured representation of the sentence, making it easier for subsequent analysis.\n",
    "\n",
    "2. **Part-of-Speech Tagging:** Part-of-speech tagging involves assigning grammatical labels (e.g., noun, verb, adjective) to each word in a sentence. This helps in understanding the syntactic structure of the sentence and disambiguating word meanings. For example, it distinguishes between the noun \"bat\" and the verb \"bat.\"\n",
    "\n",
    "3. **Parsing:** Parsing is the process of determining the syntactic structure of a sentence, typically in the form of a parse tree or dependency tree. It shows how words in a sentence relate to each other and their grammatical roles. For instance, it can reveal subject-verb-object relationships.\n",
    "\n",
    "4. **Named Entity Recognition (NER):** NER identifies and classifies named entities in a sentence, such as names of people, places, organizations, dates, and more. It is crucial for information extraction and context understanding.\n",
    "\n",
    "5. **Sentiment Analysis:** In sentiment analysis, the goal is to determine the emotional tone or sentiment expressed in a sentence, such as whether it is positive, negative, or neutral. This is valuable for understanding public opinion or customer feedback.\n",
    "\n",
    "6. **Dependency Parsing:** Dependency parsing is a technique for representing the grammatical structure of a sentence as a directed graph, where words are nodes and grammatical relationships are edges. It's valuable for understanding the relationships between words in a sentence.\n",
    "\n",
    "7. **Semantics and Word Sense Disambiguation:** These techniques aim to resolve word meanings and understand the semantics of a sentence. Word sense disambiguation helps in determining which sense of a word is intended in a given context.\n",
    "\n",
    "8. **Coreference Resolution:** Coreference resolution identifies when multiple words or phrases in a sentence refer to the same entity. For example, in \"John met Mary. He gave her a book,\" coreference resolution helps connect \"He\" to \"John\" and \"her\" to \"Mary.\"\n",
    "\n",
    "9. **Question Answering:** In question-answering systems, sentence processing is crucial for extracting relevant information from a sentence to answer a user's question.\n",
    "\n",
    "10. **Machine Translation:** Sentence processing is vital in machine translation systems to break down sentences in one language and construct equivalent sentences in another language.\n",
    "\n",
    "11. **Text Summarization:** For abstractive or extractive text summarization, sentence processing techniques are employed to identify important sentences and extract relevant information.\n",
    "\n",
    "Sentence processing is a complex and multifaceted field within NLP, involving a combination of linguistic knowledge and computational techniques. The goal is to transform text into a structured format that can be used for various NLP tasks, enabling machines to understand, interpret, and generate human language.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence', 'processed']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    \"\"\"Processes a sentence by performing the following steps:\n",
    "        1. Removing punctuation\n",
    "        2. Lowercasing the text\n",
    "        3. Tokenizing the text\n",
    "        4. Removing stop words\n",
    "        5. Lemmatizing the text\n",
    "\n",
    "    Args:\n",
    "        sentence: A string containing the sentence to be processed.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings containing the processed tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove punctuation\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "    # Lowercase the text\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Remove stop words\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    # Lemmatize the text\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "sentence = \"This is a sentence to be processed.\"\n",
    "processed_tokens = process_sentence(sentence)\n",
    "\n",
    "print(processed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is `Word Embedding`?\n",
    "\n",
    "Word embeddings are a fundamental concept in Natural Language Processing (NLP) and are used to represent words as dense, continuous-valued vectors in a high-dimensional space. Word embeddings capture the semantic and contextual information of words, making them suitable for various NLP tasks. The primary idea behind word embeddings is to map words with similar meanings or contextual usage to nearby points in the vector space. Word embeddings have become a key component in modern NLP and have significantly improved the performance of various language-related tasks. Here are some key points to understand word embeddings:\n",
    "\n",
    "1. **Distributed Representation:** Word embeddings provide a distributed representation for words, where each word is represented as a vector of real numbers. Unlike one-hot encoding, where words are represented as binary vectors, word embeddings capture more nuanced information about word relationships.\n",
    "\n",
    "2. **Semantic Similarity:** Words with similar meanings or usage contexts tend to have similar word vectors. This is because word embeddings are trained on large text corpora, allowing them to capture the semantic relationships between words. For example, in a good word embedding space, \"king\" and \"queen\" would be closer together than \"king\" and \"car.\"\n",
    "\n",
    "3. **Contextual Information:** Word embeddings capture contextual information. The meaning of a word can depend on the words that surround it in a sentence. Word embeddings are trained to consider these contextual dependencies. For instance, the word \"bank\" can have different meanings in the context of \"river bank\" or \"financial bank.\"\n",
    "\n",
    "4. **Word Embedding Models:** Several word embedding models exist, with Word2Vec, GloVe (Global Vectors for Word Representation), and FastText being some of the most popular. These models are trained on large text corpora, and they learn word embeddings based on co-occurrence statistics, predicting context words given target words or vice versa.\n",
    "\n",
    "5. **Word Embedding Dimension:** You can choose the dimensionality of word embeddings, which determines the length of the vectors. Common dimensions include 50, 100, 200, or 300. The choice of dimensionality may depend on the specific task and available resources.\n",
    "\n",
    "6. **Pre-trained Word Embeddings:** Instead of training word embeddings from scratch, you can use pre-trained word embeddings obtained from large text corpora. These embeddings are available for many languages and have been used to boost the performance of various NLP applications.\n",
    "\n",
    "7. **Word Embeddings and NLP Tasks:** Word embeddings serve as input features or pre-trained knowledge for many NLP tasks, including text classification, sentiment analysis, machine translation, and named entity recognition. They are used to improve the efficiency and effectiveness of models.\n",
    "\n",
    "8. **Word Embedding Evaluation:** The quality of word embeddings can be evaluated using intrinsic and extrinsic evaluation methods. Intrinsic evaluations measure how well embeddings capture semantic relationships (e.g., word similarity tasks), while extrinsic evaluations assess their usefulness in downstream NLP tasks.\n",
    "\n",
    "Word embeddings have become a cornerstone in NLP, facilitating the development of powerful language models like BERT, GPT, and more. These models build upon pre-trained word embeddings to capture even more nuanced contextual information and have significantly advanced the state of the art in various NLP applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.17725952  2.71517822 -0.00587937 -0.62705346  1.16898162  0.61741934\n",
      "  0.62009413 -0.57880074  0.03731495 -1.12025105]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class WordEmbedding:\n",
    "    def __init__(self, vocabulary, embedding_dim):\n",
    "        \"\"\"Initializes the word embedding.\n",
    "\n",
    "        Args:\n",
    "            vocabulary: A list of strings containing the words in the vocabulary.\n",
    "            embedding_dim: The dimension of the embedding vectors.\n",
    "        \"\"\"\n",
    "\n",
    "        self.vocabulary = vocabulary\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Initialize the embedding matrix with random values.\n",
    "        self.embedding_matrix = np.random.randn(len(vocabulary), embedding_dim)\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"Returns the embedding vector for the given word.\n",
    "\n",
    "        Args:\n",
    "            word: A string containing the word to get the embedding for.\n",
    "\n",
    "        Returns:\n",
    "            A numpy array containing the embedding vector for the given word.\n",
    "        \"\"\"\n",
    "\n",
    "        if word not in self.vocabulary:\n",
    "            raise KeyError(\"Word '%s' not found in vocabulary.\" % word)\n",
    "\n",
    "        return self.embedding_matrix[self.vocabulary.index(word)]\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "vocabulary = [\"cat\", \"dog\", \"bird\", \"fish\"]\n",
    "embedding_dim = 10\n",
    "\n",
    "word_embedding = WordEmbedding(vocabulary, embedding_dim)\n",
    "\n",
    "# Get the embedding vector for the word \"cat\".\n",
    "cat_embedding = word_embedding.get_embedding(\"cat\")\n",
    "\n",
    "print(cat_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.13148445  0.13151881  0.12148918  0.03608256  0.00664068 -0.17636096\n",
      "  0.04377532  0.29101297 -0.15758431 -0.15795347 -0.02130313 -0.23803149\n",
      "  0.07775006  0.10257284  0.17024262 -0.11945024  0.07561881 -0.04605036\n",
      " -0.07598561 -0.2583426   0.20864409  0.05027337  0.23839258  0.08950492\n",
      " -0.00207741 -0.04361648 -0.15078464  0.04011612 -0.01617661 -0.01065332\n",
      "  0.1590092  -0.0477016   0.19777317 -0.181487   -0.0091598   0.11103874\n",
      " -0.05676052  0.02131273 -0.1728234  -0.06679914  0.05130099 -0.11396933\n",
      "  0.00971357 -0.02986136  0.04378323  0.04729443 -0.13513972  0.07450062\n",
      " -0.00672828  0.15183085  0.10400049 -0.15612711 -0.08382687 -0.08972693\n",
      "  0.06481903 -0.08024552  0.01797736  0.01247341 -0.09846347 -0.05893303\n",
      " -0.04407734  0.14424768 -0.02657512  0.0418223  -0.17740534  0.1396236\n",
      "  0.13482891  0.12985158 -0.2287927   0.2821855  -0.00092568  0.04678812\n",
      "  0.18353017 -0.04521959  0.11435018  0.09481178  0.0900376   0.0298633\n",
      "  0.00390646 -0.00905123 -0.02584108  0.1497851  -0.1117153   0.1493441\n",
      " -0.13852975  0.04046867  0.04396481  0.07368866  0.11585297  0.05974779\n",
      "  0.22699499 -0.07529386 -0.12634291  0.10825546  0.14573248  0.08306377\n",
      "  0.04397481 -0.17305727 -0.00708073 -0.06943011]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Load the Brown Corpus\n",
    "corpus = brown.sents()\n",
    "\n",
    "# Create the Word2Vec model\n",
    "word2vec = gensim.models.Word2Vec(corpus, vector_size=100, window=5, min_count=5)\n",
    "\n",
    "# Get the embedding vector for the word \"cat\"\n",
    "cat_embedding = word2vec.wv.get_vector(\"cat\")\n",
    "\n",
    "print(cat_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization In Text Processing\n",
    "\n",
    "Lemmatization is a text processing technique used in Natural Language Processing (NLP) to reduce words to their base or dictionary form, known as the \"lemma.\" The primary goal of lemmatization is to transform different inflected forms of a word into a common base form so that they can be analyzed, compared, and understood more easily. Lemmatization helps in addressing word variations caused by tense, case, gender, number, and other grammatical differences. Here's how lemmatization works:\n",
    "\n",
    "1. **Lemmatization vs. Stemming:**\n",
    "   Lemmatization is often compared to stemming, another text normalization technique. While stemming reduces words to their root form, it doesn't always guarantee that the resulting \"stem\" is a valid word. Lemmatization, on the other hand, aims to reduce words to their dictionary form (lemma), ensuring that the output is a valid word. For example:\n",
    "   - Stemming: \"jumping\" -> \"jump\"\n",
    "   - Lemmatization: \"jumping\" -> \"jump\"\n",
    "\n",
    "2. **Lemmatization Process:**\n",
    "   Lemmatization involves dictionary or vocabulary lookup to map each word to its lemma. The process typically considers the word's part of speech (POS) because the lemma of a word may vary depending on whether it's used as a noun, verb, adjective, etc. For example, the lemma of \"better\" could be \"good\" when used as an adjective but \"well\" when used as an adverb.\n",
    "\n",
    "3. **POS Tagging:** To perform accurate lemmatization, it's essential to determine the part of speech of each word in the text. This is typically done using POS tagging, a process where each word is assigned a grammatical category (noun, verb, adjective, etc.) based on its context.\n",
    "\n",
    "4. **Lemmatization Algorithms:** Lemmatization is often implemented using linguistic resources like WordNet or through lemmatization algorithms that take into account the word's morphology and the POS. Popular lemmatization libraries in Python include spaCy and NLTK.\n",
    "\n",
    "5. **Example:**\n",
    "   - Lemmatization of the word \"running\":\n",
    "     - When used as a verb: \"run\"\n",
    "     - When used as a noun: \"running\"\n",
    "\n",
    "6. **Use Cases:**\n",
    "   Lemmatization is valuable in various NLP tasks, including:\n",
    "   - Information retrieval and text indexing.\n",
    "   - Text classification and sentiment analysis.\n",
    "   - Named entity recognition (NER) and part-of-speech tagging.\n",
    "   - Machine translation and information retrieval.\n",
    "   - Search engines for matching queries to documents.\n",
    "   - Text summarization and document clustering.\n",
    "\n",
    "Lemmatization helps improve the quality of text analysis and language understanding, particularly when words need to be matched, compared, or aggregated in a way that considers their grammatical variations. It is a valuable preprocessing step in NLP and can enhance the performance of various language-related tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'wa', 'running', 'and', 'eating', 'at', 'same', 'time', '.', 'He', 'ha', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hour', 'in', 'the', 'Sun', '.']\n",
      "['This', 'is', 'a', 'sentence', 'to', 'be', 'lemmatized', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatizes the given text.\n",
    "\n",
    "    Args:\n",
    "        text: A string containing the text to be lemmatized.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings containing the lemmatized tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "text = \"This is a sentence to be lemmatized.\"\n",
    "text2=\"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "lem=lemmatize_text(text2)\n",
    "lemmatized_tokens = lemmatize_text(text)\n",
    "print(lem)\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution\n",
    "\n",
    "In Natural Language Processing (NLP), a frequency distribution is a way of counting and summarizing the frequency or occurrence of words or other linguistic elements within a text corpus. It provides insights into the most common and less common words in a given dataset and is a foundational concept for various text analysis and language understanding tasks.\n",
    "\n",
    "Here's a simple implementation of frequency distribution using Python's NLTK (Natural Language Toolkit) library:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text data\n",
    "text = \"Natural Language Processing is a subfield of artificial intelligence that deals with the interaction between computers and humans using natural language. It involves various tasks like text classification, sentiment analysis, and machine translation.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Calculate the frequency distribution of words\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "# Print the 10 most common words and their frequencies\n",
    "common_words = fdist.most_common(10)\n",
    "for word, freq in common_words:\n",
    "    print(f\"{word}: {freq} times\")\n",
    "```\n",
    "\n",
    "In this implementation:\n",
    "\n",
    "1. We import the necessary modules from NLTK.\n",
    "2. We define a sample text.\n",
    "3. We tokenize the text into individual words using `word_tokenize`.\n",
    "4. We create a frequency distribution object `fdist` by passing the list of tokens.\n",
    "5. We use the `most_common()` method to retrieve the top 10 most common words and their frequencies.\n",
    "6. We print the results, which show the 10 most common words and how many times each of them appears in the text.\n",
    "\n",
    "Output:\n",
    "```\n",
    ". : 2 times\n",
    "and : 2 times\n",
    "is : 1 times\n",
    "a : 1 times\n",
    "of : 1 times\n",
    "that : 1 times\n",
    "the : 1 times\n",
    "with : 1 times\n",
    "Natural : 1 times\n",
    "Language : 1 times\n",
    "```\n",
    "\n",
    "Frequency distributions are valuable for various NLP tasks such as text summarization, document classification, identifying stopwords, and understanding the most salient terms in a text corpus. They are a fundamental tool for statistical analysis of text data and serve as a building block for more advanced language processing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and: 2 times\n",
      ".: 2 times\n",
      ",: 2 times\n",
      "Natural: 1 times\n",
      "Language: 1 times\n",
      "Processing: 1 times\n",
      "is: 1 times\n",
      "a: 1 times\n",
      "subfield: 1 times\n",
      "of: 1 times\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text data\n",
    "text = \"Natural Language Processing is a subfield of artificial intelligence that deals with the interaction between computers and humans using natural language. It involves various tasks like text classification, sentiment analysis, and machine translation.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Calculate the frequency distribution of words\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "# Print the 10 most common words and their frequencies\n",
    "common_words = fdist.most_common(10)\n",
    "for word, freq in common_words:\n",
    "    print(f\"{word}: {freq} times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count\n",
    "\n",
    "Word count is the number of words in a piece of text. It is a simple but important metric for measuring the length and complexity of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"Counts the number of words in the given text.\n",
    "\n",
    "    Args:\n",
    "        text: A string containing the text to be counted.\n",
    "\n",
    "    Returns:\n",
    "        An integer representing the number of words in the text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Count the number of tokens\n",
    "    word_count = len(tokens)\n",
    "\n",
    "    return word_count\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "text = \"This is a sentence to be counted.\"\n",
    "word_count = count_words(text)\n",
    "\n",
    "print(word_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization\n",
    "\n",
    "Text normalization is the process of transforming text into a single canonical form that it might not have had before. This is done to make text more consistent and easier to process for downstream tasks, such as machine learning and text comparison.\n",
    "\n",
    "Text normalization can involve a variety of steps, including:\n",
    "\n",
    "* **Removing punctuation:** This is typically done using regular expressions to match and remove punctuation characters.\n",
    "* **Lowercasing the text:** This is done to make the text more consistent and easier to process.\n",
    "* **Tokenizing the text:** This involves splitting the text into individual tokens, such as words and punctuation marks.\n",
    "* **Removing stop words:** Stop words are common words that do not add much meaning to text, such as \"the\", \"is\", and \"of\". These words are typically removed to make the text more concise and easier to process.\n",
    "* **Stemming or lemmatizing the text:** Stemming and lemmatization are two techniques for reducing words to their root form. Stemming is a simpler process that typically removes affixes from words, while lemmatization is a more complex process that takes into account the morphological structure of words. Which technique to use depends on the specific task at hand.\n",
    "\n",
    "In addition to these basic steps, text normalization can also involve more complex tasks, such as:\n",
    "\n",
    "* **Expanding abbreviations:** Abbreviations can be expanded to their full form, such as \"USA\" to \"United States of America\".\n",
    "* **Correcting spelling errors:** Spelling errors can be corrected using a variety of techniques, such as spell check dictionaries and language models.\n",
    "* **Converting text to a standard format:** Text can be converted to a standard format, such as Unicode or HTML.\n",
    "\n",
    "Text normalization is an important step in many text processing tasks. By normalizing text, we can make it more consistent and easier to process, which can improve the performance of downstream tasks.\n",
    "\n",
    "Here are some examples of how text normalization can be used:\n",
    "\n",
    "* **Machine learning:** Text normalization is often used to pre-process text before using it for machine learning tasks. This helps to improve the performance of machine learning models by making the text more consistent and easier to process. For example, a machine learning model that is trained to classify text into different categories may be more accurate if the text is first normalized by removing punctuation, lowercasing the text, and tokenizing the text.\n",
    "* **Text comparison:** Text normalization can also be used to standardize text for text comparison tasks. This can be useful for tasks such as finding duplicate text or matching text to a database. For example, if we want to find duplicate text in a corpus of documents, we can first normalize the text in all of the documents by removing punctuation, lowercasing the text, and tokenizing the text. This will make it easier to identify duplicate text, even if the text is written in different ways.\n",
    "\n",
    "Text normalization is a complex topic, and there is no one-size-fits-all solution. The specific steps involved in text normalization will vary depending on the specific task at hand. However, the basic steps of removing punctuation, lowercasing the text, and tokenizing the text are a good starting point for many text processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentenc', 'normal']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalizes the given text by performing the following steps:\n",
    "        1. Removing punctuation\n",
    "        2. Lowercasing the text\n",
    "        3. Tokenizing the text\n",
    "        4. Removing stop words\n",
    "        5. Stemming or lemmatizing the text\n",
    "\n",
    "    Args:\n",
    "        text: A string containing the text to be normalized.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings containing the normalized tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    # Stem or lemmatize the text\n",
    "    # Stemming is a simpler process that reduces words to their root form, while lemmatization is a more complex process that reduces words to their dictionary form.\n",
    "    # You can choose which process to use depending on your specific needs.\n",
    "    # Here is an example of how to stem the text:\n",
    "\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Here is an example of how to lemmatize the text:\n",
    "\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "\n",
    "text = \"This is a sentence to be normalized.\"\n",
    "normalized_tokens = normalize_text(text)\n",
    "\n",
    "print(normalized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is C0-Occurance Vecotr?\n",
    "\n",
    "A co-occurrence vector is a representation of the relationship between words in a text corpus. It is constructed by counting the number of times two words appear together in a given context. The context can be a predefined window size, such as the five words before and after a given word, or it can be a more complex definition of context, such as all of the words in a sentence or paragraph.\n",
    "\n",
    "Once the co-occurrence matrix is constructed, each row of the matrix represents a word, and each column of the matrix represents another word. The value of each cell in the matrix represents the number of times the two words appear together in the context.\n",
    "\n",
    "Co-occurrence vectors can be used for a variety of tasks, including:\n",
    "\n",
    "* **Word similarity:** Co-occurrence vectors can be used to measure the similarity between words. Words that appear together frequently in the context are more likely to be semantically similar.\n",
    "* **Word clustering:** Co-occurrence vectors can be used to cluster words into groups of semantically similar words. This can be useful for tasks such as topic modeling and text classification.\n",
    "* **Anomaly detection:** Co-occurrence vectors can be used to detect anomalies in text. For example, a word that appears with a set of words that it is not typically associated with may be an anomaly. This can be useful for tasks such as fraud detection and plagiarism detection.\n",
    "\n",
    "Here is an example of a co-occurrence vector for the word \"dog\":\n",
    "\n",
    "```\n",
    "dog | cat | ball | park | fetch\n",
    "------- | -------- | -------- | -------- | --------\n",
    "100      | 50        | 40        | 30        | 20\n",
    "```\n",
    "\n",
    "This co-occurrence vector shows that the word \"dog\" appears together with the words \"cat\", \"ball\", \"park\", and \"fetch\" more often than it appears with other words. This suggests that these words are semantically similar to the word \"dog\".\n",
    "\n",
    "Co-occurrence vectors are a powerful tool for representing the relationships between words in a text corpus. They can be used for a variety of tasks, including word similarity, word clustering, and anomaly detection.\n",
    "\n",
    "Here are some additional benefits of using co-occurrence vectors:\n",
    "\n",
    "* They are relatively easy to construct.\n",
    "* They can be used to represent the relationships between words in a variety of contexts.\n",
    "* They can be used for a variety of tasks, including word similarity, word clustering, and anomaly detection.\n",
    "\n",
    "However, co-occurrence vectors also have some limitations:\n",
    "\n",
    "* They can be computationally expensive to construct for large text corpora.\n",
    "* They can be sensitive to the context in which they are constructed.\n",
    "* They may not be able to capture all of the semantic relationships between words.\n",
    "\n",
    "Overall, co-occurrence vectors are a valuable tool for representing the relationships between words in a text corpus. They can be used for a variety of tasks, but it is important to be aware of their limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'construct', 'a', 'co-occurrence', 'vector']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "def construct_cooccurrence_vector(text, window_size=5):\n",
    "    \"\"\"Constructs a co-occurrence vector for the given text using NLTK.\n",
    "\n",
    "    Args:\n",
    "        text: A string containing the text to construct the co-occurrence vector for.\n",
    "        window_size: The size of the context window to use.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary mapping words to their co-occurrence vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    cooccurrence_vector = defaultdict(list)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        # Get the current word and the context words.\n",
    "        current_word = tokens[i]\n",
    "        context_words = tokens[i - window_size: i] + tokens[i + 1: i + window_size + 1]\n",
    "\n",
    "        for context_word in context_words:\n",
    "            # Increment the co-occurrence count for the current word and the context word.\n",
    "            cooccurrence_vector[current_word].append(context_word)\n",
    "\n",
    "    return cooccurrence_vector\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "text = \"This is a sentence to construct a co-occurrence vector for.\"\n",
    "cooccurrence_vector = construct_cooccurrence_vector(text)\n",
    "\n",
    "# Print the co-occurrence vector for the word \"sentence\".\n",
    "print(cooccurrence_vector[\"sentence\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('e', 'n'), 3), (('i', 's'), 2), (('s', ' '), 2), ((' ', 'a'), 2), (('a', ' '), 2), (('n', 'c'), 2), (('c', 'e'), 2), (('e', ' '), 2), (('t', 'o'), 2), ((' ', 'c'), 2)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Create a nltk.Text object from the text.\n",
    "text = nltk.Text(\"This is a sentence to construct a co-occurrence vector for.\")\n",
    "\n",
    "# Create a nltk.collocations.BigramCollocationFinder object.\n",
    "bigram_collocation_finder = nltk.collocations.BigramCollocationFinder.from_words(text.tokens)\n",
    "\n",
    "# Identify the bigrams that co-occur more frequently than expected by chance.\n",
    "bigrams = bigram_collocation_finder.ngram_fd.most_common()\n",
    "\n",
    "# Print the top 10 bigrams.\n",
    "print(bigrams[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thank You!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
