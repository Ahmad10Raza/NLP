{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Natural Language Toolkit (NLTK)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction to `nltk`\n",
    "\n",
    "The Natural Language Toolkit, often referred to as NLTK, is a comprehensive library for working with human language data using Python. It provides easy-to-use interfaces to over 50 corpora and lexical resources, as well as a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, among other things. NLTK is one of the most popular and widely used libraries for natural language processing (NLP) in the Python ecosystem.\n",
    "\n",
    "Key features and components of NLTK include:\n",
    "\n",
    "1. **Corpora:** NLTK includes a vast collection of linguistic corpora, such as the Penn Treebank, WordNet, and various text collections. These corpora serve as valuable resources for linguistic research and NLP tasks.\n",
    "\n",
    "2. **Text Processing Libraries:** NLTK provides a wide range of text processing tools and modules, including tokenization, stemming, lemmatization, part-of-speech tagging, parsing, and more. These tools enable you to preprocess and analyze text data effectively.\n",
    "\n",
    "3. **Machine Learning:** NLTK includes utilities and algorithms for text classification, sentiment analysis, and other machine learning-based NLP tasks.\n",
    "\n",
    "4. **Linguistic Resources:** The library offers access to lexical resources like WordNet, which is a large lexical database of English, and various language grammars and parsers.\n",
    "\n",
    "5. **Text Corpora and Lexical Resources:** NLTK provides access to a variety of text corpora and lexical resources, including dictionaries and thesauri.\n",
    "\n",
    "6. **Natural Language Processing and Linguistics Algorithms:** It includes various algorithms for tasks such as parsing, semantic reasoning, and machine translation.\n",
    "\n",
    "7. **Visualization and Tools:** NLTK offers tools for visualization and exploration of linguistic data.\n",
    "\n",
    "8. **Community and Resources:** NLTK has a large and active user community, along with extensive documentation and educational resources. It's widely used in academia and industry for NLP research and applications.\n",
    "\n",
    "NLTK is a versatile library that is commonly used for various NLP tasks, including text classification, sentiment analysis, information extraction, and text generation. It serves as a valuable resource for researchers, developers, and data scientists working with natural language data in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpora(Corpus)\n",
    "\n",
    "In NLTK (Natural Language Toolkit), a corpus (plural: corpora) refers to a large and structured collection of text or speech data. Corpora in NLTK are used for various natural language processing (NLP) tasks, including linguistic research, text analysis, and the development of NLP models and algorithms. These corpora are often used as training and testing data for NLP tasks, and they provide researchers and practitioners with a broad range of textual resources for different languages and domains.\n",
    "\n",
    "NLTK includes various built-in corpora that cover different domains, languages, and types of text data. Some of the most commonly used corpora in NLTK include:\n",
    "\n",
    "1. **Gutenberg Corpus:** A collection of classic literary texts, such as novels and essays, from the Project Gutenberg digital library.\n",
    "\n",
    "2. **Brown Corpus:** A corpus of American English text from diverse sources, classified into numerous genres and used for linguistic research.\n",
    "\n",
    "3. **Inaugural Address Corpus:** A collection of U.S. presidential inaugural addresses, useful for studying the language used in political speeches.\n",
    "\n",
    "4. **WordNet:** While not a text corpus, WordNet is a lexical database that NLTK provides access to. It's a resource for looking up word meanings, synonyms, antonyms, and other lexical information.\n",
    "\n",
    "5. **Penn Treebank Corpus:** A collection of newspaper text with part-of-speech tagging, syntactic parsing, and other linguistic annotations.\n",
    "\n",
    "6. **Reuters Corpus:** A collection of news articles from the Reuters news agency, often used for text classification and information retrieval tasks.\n",
    "\n",
    "7. **Movie Reviews Corpus:** A collection of movie reviews categorized as positive or negative, frequently used for sentiment analysis and text classification.\n",
    "\n",
    "8. **Chat-80 Data:** A corpus of chat-room-style conversations.\n",
    "\n",
    "9. **Web Text Corpus:** A corpus containing text from a variety of web sources.\n",
    "\n",
    "These corpora serve different purposes in linguistic research and NLP tasks, such as text classification, language modeling, sentiment analysis, and more. NLTK provides tools and methods to access and work with these corpora, making it a valuable resource for NLP practitioners and researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing\n",
    "\n",
    "Data pre-processing is the process of making the machine understand things better or making the input more machine understandable. Some standard practices for doing that are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of breaking a text into individual words or \"tokens.\" NLTK (Natural Language Toolkit) provides various methods for tokenizing text in Python. Here's how you can tokenize text using NLTK:\n",
    "\n",
    "1. **Using NLTK's Default Tokenizer:**\n",
    "\n",
    "   NLTK comes with a default tokenizer called `word_tokenize`, which can be used to split text into words.\n",
    "\n",
    "   ```python\n",
    "   from nltk.tokenize import word_tokenize\n",
    "\n",
    "   text = \"Tokenization is the process of breaking down text into words or phrases.\"\n",
    "   tokens = word_tokenize(text)\n",
    "\n",
    "   print(tokens)\n",
    "   ```\n",
    "\n",
    "   Output:\n",
    "   ```\n",
    "   ['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'into', 'words', 'or', 'phrases', '.']\n",
    "   ```\n",
    "\n",
    "2. **Using NLTK's Sentence Tokenizer:**\n",
    "\n",
    "   If you want to split text into sentences, you can use NLTK's `sent_tokenize`.\n",
    "\n",
    "   ```python\n",
    "   from nltk.tokenize import sent_tokenize\n",
    "\n",
    "   text = \"This is the first sentence. And this is the second one!\"\n",
    "   sentences = sent_tokenize(text)\n",
    "\n",
    "   print(sentences)\n",
    "   ```\n",
    "\n",
    "   Output:\n",
    "   ```\n",
    "   ['This is the first sentence.', 'And this is the second one!']\n",
    "   ```\n",
    "\n",
    "3. **Custom Tokenization:**\n",
    "\n",
    "   You can also create a custom tokenizer using regular expressions to split text based on specific patterns. For example, you can tokenize text based on spaces and punctuation.\n",
    "\n",
    "   ```python\n",
    "   import re\n",
    "\n",
    "   text = \"Custom tokenization can be done with regular expressions. For example, split text based on spaces and punctuation!\"\n",
    "\n",
    "   # Tokenize based on spaces and punctuation\n",
    "   tokens = re.split(r'\\s+|[,;.!]', text)\n",
    "\n",
    "   # Remove empty strings\n",
    "   tokens = [token for token in tokens if token]\n",
    "\n",
    "   print(tokens)\n",
    "   ```\n",
    "\n",
    "   Output:\n",
    "   ```\n",
    "   ['Custom', 'tokenization', 'can', 'be', 'done', 'with', 'regular', 'expressions', 'For', 'example', 'split', 'text', 'based', 'on', 'spaces', 'and', 'punctuation']\n",
    "   ```\n",
    "\n",
    "The choice of tokenizer depends on your specific NLP task and the characteristics of the text you are working with. NLTK provides flexibility and allows you to use the tokenizer that best fits your needs, whether it's the default tokenizer, a sentence tokenizer, or a custom tokenizer based on regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'pledge', 'to', 'be', 'a', 'data', 'scientist', 'one', 'day']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "data = \"I pledge to be a data scientist one day\"\n",
    "tokenized_text=word_tokenize(data)\n",
    "print(tokenized_text)\n",
    "print(type(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cake is a form of sweet food made from flour, sugar, and other ingredients, that is usually baked.In their oldest forms, cakes were modifications of bread, but cakes now cover a wide range of preparations that can be simple or elaborate, and that share features with other desserts such as pastries, meringues, custards, and pies.The most commonly used cake ingredients include flour, sugar, eggs, butter or oil or margarine, a liquid, and leavening agents, such as baking soda or baking powder.', 'Common additional ingredients and flavourings include dried, candied, or fresh fruit, nuts, cocoa, and extracts such as vanilla, with numerous substitutions for the primary ingredients.Cakes can also be filled with fruit preserves, nuts or dessert sauces (like pastry cream), iced with buttercream or other icings, and decorated with marzipan, piped borders, or candied fruit.']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "para=\"\"\"Cake is a form of sweet food made from flour, sugar, and other ingredients, that is usually baked.In their oldest forms, cakes were modifications of bread, but cakes now cover a wide range of preparations that can be simple or elaborate, and that share features with other desserts such as pastries, meringues, custards, and pies.The most commonly used cake ingredients include flour, sugar, eggs, butter or oil or margarine, a liquid, and leavening agents, such as baking soda or baking powder. Common additional ingredients and flavourings include dried, candied, or fresh fruit, nuts, cocoa, and extracts such as vanilla, with numerous substitutions for the primary ingredients.Cakes can also be filled with fruit preserves, nuts or dessert sauces (like pastry cream), iced with buttercream or other icings, and decorated with marzipan, piped borders, or candied fruit.\"\"\"\n",
    "tokenized_para=sent_tokenize(para)\n",
    "print(tokenized_para)\n",
    "print(type(tokenized_para))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Punctuation Removal\n",
    "\n",
    "Removing punctuation from text is a common preprocessing step in natural language processing (NLP) tasks. Punctuation removal helps simplify text and can be useful for various NLP tasks like text classification, text analysis, and text mining. You can remove punctuation from text in Python using various methods, including regular expressions and string manipulation.\n",
    "\n",
    "Here's how to remove punctuation from text using Python:\n",
    "\n",
    "1. **Using Regular Expressions:**\n",
    "\n",
    "   You can use the `re` library to remove punctuation using regular expressions. In this example, we'll remove all non-alphanumeric characters (i.e., remove everything that is not a letter or a number):\n",
    "\n",
    "   ```python\n",
    "   import re\n",
    "\n",
    "   text = \"Hello, World! This is an example text with some punctuation.\"\n",
    "\n",
    "   # Remove non-alphanumeric characters using regular expression\n",
    "   text_without_punctuation = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "\n",
    "   print(text_without_punctuation)\n",
    "   ```\n",
    "\n",
    "   Output:\n",
    "   ```\n",
    "   Hello World This is an example text with some punctuation\n",
    "   ```\n",
    "\n",
    "2. **Using String Manipulation:**\n",
    "\n",
    "   You can also remove punctuation by iterating through each character in the text and keeping only the characters that are letters or spaces:\n",
    "\n",
    "   ```python\n",
    "   text = \"Hello, World! This is an example text with some punctuation.\"\n",
    "\n",
    "   # Remove punctuation using string manipulation\n",
    "   text_without_punctuation = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "\n",
    "   print(text_without_punctuation)\n",
    "   ```\n",
    "\n",
    "   Output:\n",
    "   ```\n",
    "   Hello World This is an example text with some punctuation\n",
    "   ```\n",
    "\n",
    "3. **Using the `string` Module:**\n",
    "\n",
    "   Python's `string` module provides a string of all punctuation characters. You can use this module to remove punctuation from text:\n",
    "\n",
    "   ```python\n",
    "   import string\n",
    "\n",
    "   text = \"Hello, World! This is an example text with some punctuation.\"\n",
    "\n",
    "   # Remove punctuation using the string module\n",
    "   translator = str.maketrans('', '', string.punctuation)\n",
    "   text_without_punctuation = text.translate(translator)\n",
    "\n",
    "   print(text_without_punctuation)\n",
    "   ```\n",
    "\n",
    "   Output:\n",
    "   ```\n",
    "   Hello World This is an example text with some punctuation\n",
    "   ```\n",
    "\n",
    "Choose the method that best fits your specific NLP task and text data. Punctuation removal is often an essential step in text preprocessing to ensure that the text is in a clean and suitable format for analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', 'I', 'am', 'excited', 'to', 'learn', 'data', 'science']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "result = tokenizer.tokenize(\"Wow! I am excited to learn data science\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stop Words Removal\n",
    "Stop words are words which occur frequently in a corpus. e.g a, an, the, in. Frequently occurring words are removed from the corpus for the sake of text-normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cake', 'is', 'a', 'form', 'of', 'sweet', 'food', 'made', 'from', 'flour', ',', 'sugar', ',', 'and', 'other', 'ingredients', ',', 'that', 'is', 'usually', 'baked', '.', 'In', 'their', 'oldest', 'forms', ',', 'cakes', 'were', 'modifications', 'of', 'bread', ',', 'but', 'cakes', 'now', 'cover', 'a', 'wide', 'range', 'of', 'preparations', 'that', 'can', 'be', 'simple', 'or', 'elaborate', ',', 'and', 'that', 'share', 'features', 'with', 'other', 'desserts', 'such', 'as', 'pastries', ',', 'meringues', ',', 'custards', ',', 'and', 'pies', '.']\n",
      "['Cake', 'form', 'sweet', 'food', 'made', 'flour', ',', 'sugar', ',', 'ingredients', ',', 'usually', 'baked', '.', 'In', 'oldest', 'forms', ',', 'cakes', 'modifications', 'bread', ',', 'cakes', 'cover', 'wide', 'range', 'preparations', 'simple', 'elaborate', ',', 'share', 'features', 'desserts', 'pastries', ',', 'meringues', ',', 'custards', ',', 'pies', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "to_be_removed = set(stopwords.words('english'))\n",
    "para=\"\"\"Cake is a form of sweet food made from flour, sugar, and other ingredients, that is usually baked.\n",
    "In their oldest forms, cakes were modifications of bread, but cakes now cover a wide range of preparations \n",
    "that can be simple or elaborate, and that share features with other desserts such as pastries, meringues, custards, \n",
    "and pies.\"\"\"\n",
    "tokenized_para=word_tokenize(para)\n",
    "print(tokenized_para)\n",
    "modified_token_list=[word for word in tokenized_para if not word in to_be_removed]\n",
    "print(modified_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stemming\n",
    "It is reduction of inflection from words. Words with same origin will get reduced to a form which may or may not be a word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cake', 'is', 'a', 'form', 'of', 'sweet', 'food', 'made', 'from', 'flour', ',', 'sugar', ',', 'and', 'other', 'ingredi', ',', 'that', 'is', 'usual', 'baked.in', 'their', 'oldest', 'form', ',', 'cake', 'were', 'modif', 'of', 'bread', ',', 'but', 'cake', 'now', 'cover', 'a', 'wide', 'rang', 'of', 'prepar', 'that', 'can', 'be', 'simpl', 'or', 'elabor', ',', 'and', 'that', 'share', 'featur', 'with', 'other', 'dessert', 'such', 'as', 'pastri', ',', 'meringu', ',', 'custard', ',', 'and', 'pie', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "content = \"\"\"Cake is a form of sweet food made from flour, sugar, and other ingredients, that is usually baked.In their oldest forms, cakes were modifications of bread, but cakes now cover a wide range of preparations \n",
    "that can be simple or elaborate, and that share features with other desserts such as pastries, meringues, custards, and pies.\"\"\"\n",
    "tk_content=word_tokenize(content)\n",
    "stemmed_words = [stemmer.stem(i) for i in tk_content] \n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cak', 'is', 'a', 'form', 'of', 'sweet', 'food', 'mad', 'from', 'flo', ',', 'sug', ',', 'and', 'oth', 'ingredy', ',', 'that', 'is', 'us', 'bak', '.', 'in', 'their', 'oldest', 'form', ',', 'cak', 'wer', 'mod', 'of', 'bread', ',', 'but', 'cak', 'now', 'cov', 'a', 'wid', 'rang', 'of', 'prep', 'that', 'can', 'be', 'simpl', 'or', 'elab', ',', 'and', 'that', 'shar', 'feat', 'with', 'oth', 'dessert', 'such', 'as', 'pastry', ',', 'meringu', ',', 'custard', ',', 'and', 'pie', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = LancasterStemmer()\n",
    "content = \"\"\"Cake is a form of sweet food made from flour, sugar, and other ingredients, that is usually baked.\n",
    "In their oldest forms, cakes were modifications of bread, but cakes now cover a wide range of preparations \n",
    "that can be simple or elaborate, and that share features with other desserts such as pastries, meringues, custards, \n",
    "and pies.\"\"\"\n",
    "\n",
    "tk_content=word_tokenize(content)\n",
    "stemmed_words = [stemmer.stem(i) for i in tk_content]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "It is another process of reducing inflection from words. The way its different from stemming is that it reduces words to their origins which have actual meaning. Stemming sometimes generates words which are not even words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cake', 'is', 'a', 'form', 'of', 'sweet', 'food', 'made', 'from', 'flour', ',', 'sugar', ',', 'and', 'other', 'ingredient', ',', 'that', 'is', 'usually', 'baked', '.', 'In', 'their', 'oldest', 'form', ',', 'cake', 'were', 'modification', 'of', 'bread', ',', 'but', 'cake', 'now', 'cover', 'a', 'wide', 'range', 'of', 'preparation', 'that', 'can', 'be', 'simple', 'or', 'elaborate', ',', 'and', 'that', 'share', 'feature', 'with', 'other', 'dessert', 'such', 'a', 'pastry', ',', 'meringue', ',', 'custard', ',', 'and', 'pie', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "content = \"\"\"Cake is a form of sweet food made from flour, sugar, and other ingredients, that is usually baked.\n",
    "In their oldest forms, cakes were modifications of bread, but cakes now cover a wide range of preparations \n",
    "that can be simple or elaborate, and that share features with other desserts such as pastries, meringues, custards, \n",
    "and pies.\"\"\"\n",
    "\n",
    "tk_content=word_tokenize(content)\n",
    "lemmatized_words = [lemmatizer.lemmatize(i) for i in tk_content] \n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "POS tagging is the process of identifying parts of speech of a sentence. It is able to identify nouns, pronouns, adjectives etc. in a sentence and assigns a POS token to each word. There are different methods to tag, but we will be using the universal style of tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Cake', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('form', 'NOUN'), ('of', 'ADP'), ('sweet', 'ADJ'), ('food', 'NOUN'), ('made', 'VERB'), ('from', 'ADP'), ('flour', 'NOUN'), (',', '.'), ('sugar', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('other', 'ADJ'), ('ingredients', 'NOUN'), (',', '.'), ('that', 'DET'), ('is', 'VERB'), ('usually', 'ADV'), ('baked', 'VERB'), ('.', '.')], [('In', 'ADP'), ('their', 'PRON'), ('oldest', 'ADJ'), ('forms', 'NOUN'), (',', '.'), ('cakes', 'NOUN'), ('were', 'VERB'), ('modifications', 'NOUN'), ('of', 'ADP'), ('bread', 'NOUN'), (',', '.'), ('but', 'CONJ'), ('cakes', 'NOUN'), ('now', 'ADV'), ('cover', 'VERB'), ('a', 'DET'), ('wide', 'ADJ'), ('range', 'NOUN'), ('of', 'ADP'), ('preparations', 'NOUN'), ('that', 'DET'), ('can', 'VERB'), ('be', 'VERB'), ('simple', 'ADJ'), ('or', 'CONJ'), ('elaborate', 'ADJ'), (',', '.'), ('and', 'CONJ'), ('that', 'ADP'), ('share', 'NOUN'), ('features', 'NOUN'), ('with', 'ADP'), ('other', 'ADJ'), ('desserts', 'NOUN'), ('such', 'ADJ'), ('as', 'ADP'), ('pastries', 'NOUN'), (',', '.'), ('meringues', 'NOUN'), (',', '.'), ('custards', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('pies', 'NOUN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "content = \"\"\"Cake is a form of sweet food made from flour, sugar, and other ingredients, that is usually baked.\n",
    "In their oldest forms, cakes were modifications of bread, but cakes now cover a wide range of preparations \n",
    "that can be simple or elaborate, and that share features with other desserts such as pastries, meringues, custards, \n",
    "and pies.\"\"\"\n",
    "words= [word_tokenize(i) for i in sent_tokenize(content)]\n",
    "pos_tag= [nltk.pos_tag(i,tagset=\"universal\") for i in words]\n",
    "print(pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Chunking\n",
    "Chunking also known as shallow parsing, is practically a method in NLP applied to POS tagged data to gain further insights from it. It is done by grouping certain words on the basis of a pre-defined rule. The text is then parsed according to the rule to group data for phrase creation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Cake/NNP\n",
      "  is/VBZ\n",
      "  (NP a/DT form/NN)\n",
      "  of/IN\n",
      "  (NP sweet/JJ food/NN)\n",
      "  made/VBN\n",
      "  from/IN\n",
      "  (NP flour/NN)\n",
      "  ,/,\n",
      "  (NP sugar/NN)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  other/JJ\n",
      "  ingredients/NNS\n",
      "  ,/,\n",
      "  that/DT\n",
      "  is/VBZ\n",
      "  usually/RB\n",
      "  baked/VBN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "content = \"Cake is a form of sweet food made from flour, sugar, and other ingredients, that is usually baked.\"\n",
    "tokenized_text = nltk.word_tokenize(content)\n",
    "tagged_token = nltk.pos_tag(tokenized_text)\n",
    "grammer = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "phrases = nltk.RegexpParser(grammer)\n",
    "result = phrases.parse(tagged_token)\n",
    "print(result)\n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "Word Embeddings is a NLP technique in which we try to capture the context, semantic meaning and inter relation of words with each other. It is done by creation of a word vector. Word vectors when projected upon a vector space can also show similarity between words.The technique or word embeddings which we discuss here today is Word-to-vec. We would be doing so with the help of Gensim which is another cool library like NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    [\"I\", \"love\", \"machine\", \"learning\"],\n",
    "    [\"Word\", \"embeddings\", \"are\", \"important\"],\n",
    "    [\"NLTK\", \"is\", \"a\", \"natural\", \"language\", \"toolkit\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Save the model for later use\n",
    "model.save(\"word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'machine': [ 9.7702928e-03  8.1651136e-03  1.2809718e-03  5.0975787e-03\n",
      "  1.4081288e-03 -6.4551616e-03 -1.4280510e-03  6.4491653e-03\n",
      " -4.6173059e-03 -3.9930656e-03  4.9244044e-03  2.7130984e-03\n",
      " -1.8479753e-03 -2.8769434e-03  6.0107317e-03 -5.7167388e-03\n",
      " -3.2367026e-03 -6.4878250e-03 -4.2346325e-03 -8.5809948e-03\n",
      " -4.4697891e-03 -8.5112294e-03  1.4037776e-03 -8.6181965e-03\n",
      " -9.9166557e-03 -8.2016252e-03 -6.7726658e-03  6.6805850e-03\n",
      "  3.7845564e-03  3.5616636e-04 -2.9579818e-03 -7.4283206e-03\n",
      "  5.3341867e-04  4.9989222e-04  1.9561886e-04  8.5259555e-04\n",
      "  7.8633073e-04 -6.8160298e-05 -8.0070542e-03 -5.8702733e-03\n",
      " -8.3829118e-03 -1.3120425e-03  1.8206370e-03  7.4171280e-03\n",
      " -1.9634271e-03 -2.3252917e-03  9.4871549e-03  7.9704521e-05\n",
      " -2.4045217e-03  8.6048469e-03  2.6870037e-03 -5.3439722e-03\n",
      "  6.5881060e-03  4.5101536e-03 -7.0544672e-03 -3.2317400e-04\n",
      "  8.3448651e-04  5.7473574e-03 -1.7176545e-03 -2.8065301e-03\n",
      "  1.7484308e-03  8.4717153e-04  1.1928272e-03 -2.6342822e-03\n",
      " -5.9857843e-03  7.3229838e-03  7.5873756e-03  8.2963575e-03\n",
      " -8.5988473e-03  2.6364254e-03 -3.5599626e-03  9.6204039e-03\n",
      "  2.9037679e-03  4.6411133e-03  2.3856151e-03  6.6084778e-03\n",
      " -5.7432903e-03  7.8944126e-03 -2.4109220e-03 -4.5618857e-03\n",
      " -2.0609903e-03  9.7335577e-03 -6.8565905e-03 -2.1917201e-03\n",
      "  7.0009995e-03 -5.5749417e-05 -6.2949671e-03 -6.3935257e-03\n",
      "  8.9403950e-03  6.4295758e-03  4.7735930e-03 -3.2620477e-03\n",
      " -9.2676198e-03  3.7868882e-03  7.1605504e-03 -5.6328895e-03\n",
      " -7.8650126e-03 -2.9727400e-03 -4.9318983e-03 -2.3151112e-03]\n",
      "Most similar words to 'machine': [('natural', 0.17272792756557465), ('NLTK', 0.16694682836532593), ('are', 0.11117952316999435)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the Word2Vec model\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# Explore the embeddings\n",
    "vector = model.wv[\"machine\"]\n",
    "similar_words = model.wv.most_similar(\"machine\", topn=3)\n",
    "\n",
    "print(\"Vector for 'machine':\", vector)\n",
    "print(\"Most similar words to 'machine':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thank You!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
